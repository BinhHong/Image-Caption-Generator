{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03869865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ben√∂tigte Bibliotheken importieren\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "#from glob import glob\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import load_img, img_to_array, pad_sequences, to_categorical,plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,LSTM,Embedding,Dropout,add\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c718f751",
   "metadata": {},
   "source": [
    "# Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "211e72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to working directory\n",
    "working_dir = os.getcwd()\n",
    "# path to data\n",
    "data_path=os.path.join(os.getcwd(), 'Data')\n",
    "# path to images\n",
    "image_dir = os.path.join(data_path, 'Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2df44c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Binh_Hong_Ngoc\\\\Meine Daten\\\\Programming\\\\My Projects\\\\Image Caption Generator'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e613ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Binh_Hong_Ngoc\\\\Meine Daten\\\\Programming\\\\My Projects\\\\Image Caption Generator\\\\Data'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910d1f1",
   "metadata": {},
   "source": [
    "We will use the pre-trained VGG16 model. It is worth noting that it ends with a Dense layer, which is responsible for classification tasks. So for feature extraction, I'm going to remove the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ae21dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vgg16 model\n",
    "model = VGG16()\n",
    "# remove the last layer\n",
    "model = Model(inputs=model.inputs,outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2b0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134260544 (512.16 MB)\n",
      "Trainable params: 134260544 (512.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb105f",
   "metadata": {},
   "source": [
    "In the next step I will extract the image features from the VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fdb6a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ef2e2c601549efade958c971c99e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a dictionary to save the image features\n",
    "features = {}\n",
    "# extract image features\n",
    "for img_name in tqdm(os.listdir(image_dir)): # tqdm for visualization of the process\n",
    "    img_path = image_dir + '/' + img_name\n",
    "    # The default input size for this model is 224 x 224. So we need to reshape the images\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    # Reshape image to model.    \n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # preprocessing images for VGG\n",
    "    image = preprocess_input(image)\n",
    "    # extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    # image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "    # save feature\n",
    "    features[image_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4159ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save image features in \"Data\" folder using pickle\n",
    "pickle.dump(features, open(os.path.join(data_path, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f2de0",
   "metadata": {},
   "source": [
    "# Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7066497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text file\n",
    "with open(os.path.join(data_path, 'Captions\\captions.txt'), 'r') as f:\n",
    "    next(f) # The first line of captions.txt is \"image,caption\". That's why we skip this line\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c6f87",
   "metadata": {},
   "source": [
    "If we take a closer look at `captions_doc`, we can see that for each image there are corresponding 5 descriptions, which look like this:\n",
    "\n",
    "`1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\\n1000268201_693b08cb0e.jpg,A girl going into a wooden building .\\n1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\\n1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .\\n1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with keys=image_id and values=captions\n",
    "mapping = {}\n",
    "\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split by comma\n",
    "    tokens = line.split(',')\n",
    "    # remove all lines with less than 2 tokens, which may not have enough information\n",
    "    if len(tokens) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    # Remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # \n",
    "    caption = \" \".join(caption)\n",
    "    # create a list of captions\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    # add caption\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 examples\n",
    "npic = 5\n",
    "target_size = (299, 299)\n",
    "path = os.path.join(data_path, 'Images/')\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "\n",
    "count = 1\n",
    "for img in os.listdir(path)[:npic]:\n",
    "    \n",
    "    filename = path + img\n",
    "    captions = list(mapping[img.split(\".\")[0]])\n",
    "    image_load = load_img(filename, target_size=target_size)\n",
    "    \n",
    "    ax = fig.add_subplot(npic, 2, count, xticks=[], yticks=[])\n",
    "    ax.imshow(image_load)\n",
    "    count += 1\n",
    "    \n",
    "    ax = fig.add_subplot(npic, 2, count)\n",
    "    plt.axis('off')\n",
    "    ax.plot()\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, len(captions))\n",
    "    for i, caption in enumerate(captions):\n",
    "        ax.text(0, i, caption, fontsize=20)\n",
    "    count += 1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
